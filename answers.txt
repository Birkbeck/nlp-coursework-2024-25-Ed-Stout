Answers to the text questions go here.

1d:
The Flesch-Kincaid Score (FKS) estimates readability using only sentence length and word syllable count. While useful for a rough guide, it ignores many factors that affect how difficult a text actually is to understand.
Firstly, FKS doesn’t account for the conceptual difficulty of words, or for lexical and syntactic ambiguity. Lexical ambiguity is when a word has multiple meanings e.g. a bat can mean an animal or sports equipment. Syntactic ambiguity is when a sentence can be interpreted in different ways e.g. “I shot a man with a telescope” could mean either person had the telescope, or that “shot” refers to a photo rather than a weapon. Word length and syllable count do not take this into account meaning important complexities of meaning within a text are not captured by FKS.
Secondly, FKS was designed for English and assumes longer words are harder. This doesn’t always hold across languages e.g. in German, compound nouns are long but conceptually simple. Even in English, some long words are easy: everywhere (3 syllables, 10 letters) is common and simple, while heuristic (also 3 syllables, 10 letters) is more conceptually advanced, but FKS treats them the same.

2f:
My function broadly turns the raw speeches into lemmatised tokens ready for analysis. Firstly, it turns the text into lowercase and removes all punctuation, by using the re.sub(r'[^\w\s]', '', text.lower() functions. This standardises the text input, making words directly comparable, and reduces unnecessary vocabulary created by punctuation. Secondly, it passes the text to the SpaCy tokenizer (en_core_web_sm), which splits the text into tokens and assigns each a part-of-speech (POS) tag. Thirdly, the for loop goes through every token in the doc and only keeps nouns, adjectives, verbs, adverbs, and a list of politically significant words. The politically chosen words were selected (E.g. Brexit, NHS, HS2) because it is believed the frequency of their use may indicate some partisan biases, which would be a strong predictor of party affiliation. Words are also lemmatized in order to return words to their dictionary form, this prevents synonyms being treated as different words despite them probably having the same meaning (e.g. “running”, “run”, “runs”).

The tokenizer was tuned by adjusting many preprocessing parameters, such as stop-word removal, use of minimum and maximum document frequency (min-df and max-df), bigrams and trigrams, stop word removal, POS filtering, and changing the source text lowercase and removing punctuation. I added Naive Bayes as the question asked for three classifiers to be used. 

The original scores were: Random Forest 0.448, linear SVM 0.575, and Naive Bayes 0.377. The best classification performance was achieved with a tokenizer including POS filtering, lemmatization, including bigrams and trigrams, and changing source text to lowercase and removing punctuation. This improved the SVM classifier score to an F1 score of 0.667, a big improvement from the original F1 score of 0.575. This means that the classifier is significantly better at making predictions for the party affiliation based on the speeches provided. This combination wasn’t the best for the Random Forest and Naive Bayes models, which achieved 0.473 and 0.405 scores respectively under different scenarios. However, these are both increases from the original F1 scores of 0.448 for Random Forest and 0.377 for Naive Bayes. 

The SVM accuracy score doesn’t tell the whole story though. The predictions for Labour ( 0.7 to 0.73), Conservatives (the same at 0.87) and Scottish National Party (0.63 to 0.64) all improved or remained high, but the Liberal Democrats score decreased from an F1 of 0.14 to 0.06. This is because feature reduction means that terms which occur infrequently in parts of the corpus are likely to be excluded, meaning that words and vocabulary exclusive to the Liberal Democrats are more likely to be excluded, as it is the smallest class. The model failed to capture distinctive words and vocabulary associated with Liberal Democrat party affiliation as a result. I tried to mitigate this by adjusting the SVM regularisation parameter (C) to increase sensitivity to smaller classes, but this negatively impacted both the overall and Lib Dem specific scores. Future improvements could be done to the feature reduction process to improve its score, such as preserving key terms which are unique to parties underrepresented in the corpus.
